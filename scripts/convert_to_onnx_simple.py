#!/usr/bin/env python3
"""
Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë²„ì „)
"""

import argparse
import os
import sys

def convert_to_onnx(model_path, output_path):
    """Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    
    # TensorFlow/Keras ë¡œë“œ
    import tensorflow as tf
    from tensorflow import keras
    
    model = keras.models.load_model(model_path)
    
    print(f"ëª¨ë¸ ì •ë³´:")
    print(f"  ì…ë ¥ shape: {model.input_shape}")
    print(f"  ì¶œë ¥ shape: {model.output_shape}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # ONNX ë³€í™˜ ì‹œë„
    print(f"\nONNX ë³€í™˜ ì‹œë„ ì¤‘...")
    try:
        import keras2onnx
        import onnx
        
        print("keras2onnxë¡œ ë³€í™˜ ì¤‘...")
        onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=13)
        onnx.save_model(onnx_model, output_path)
        
        size = os.path.getsize(output_path)
        print(f"\nâœ… ONNX ë³€í™˜ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
        print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {size:,} bytes ({size/1024:.2f} KB)")
        
        # ONNX ëª¨ë¸ ê²€ì¦
        try:
            onnx.checker.check_model(onnx_model)
            print("âœ… ONNX ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  ONNX ëª¨ë¸ ê²€ì¦ ì¤‘ ê²½ê³ : {e}")
            
    except Exception as e:
        print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
        print(f"\nğŸ’¡ ëŒ€ì•ˆ: SavedModel í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        print(f"   TensorFlow.jsì—ì„œë„ SavedModelì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return saved_model_path
    
    return output_path


def main():
    parser = argparse.ArgumentParser(description='Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜')
    parser.add_argument('--model', type=str, default='model.h5',
                        help='ì…ë ¥ Keras ëª¨ë¸ íŒŒì¼ (default: model.h5)')
    parser.add_argument('--output', type=str, default='public/model/model.onnx',
                        help='ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ (default: public/model/model.onnx)')
    
    args = parser.parse_args()
    
    result = convert_to_onnx(args.model, args.output)
    
    if result.endswith('.onnx'):
        print(f"\nğŸ“ ONNX.js ì‚¬ìš© ë°©ë²•:")
        print(f"   1. npm install onnxruntime-web")
        print(f"   2. ë˜ëŠ” CDN: <script src='https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js'></script>")
        print(f"\n   ëª¨ë¸ ë¡œë“œ ì½”ë“œ:")
        print(f"   const session = await ort.InferenceSession.create('./model/model.onnx');")
        print(f"   const results = await session.run(feed);")


if __name__ == '__main__':
    main()

