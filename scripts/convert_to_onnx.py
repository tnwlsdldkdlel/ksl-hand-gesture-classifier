#!/usr/bin/env python3
"""
Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import os
import sys

try:
    import tensorflow as tf
    from tensorflow import keras
    import onnx
    try:
        import keras2onnx
    except ImportError:
        import tf2onnx
        keras2onnx = None
except ImportError as e:
    print(f"í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
    print("  pip3 install tensorflow onnx keras2onnx")
    sys.exit(1)


def convert_to_onnx(model_path, output_path, opset=13):
    """Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    model = keras.models.load_model(model_path)
    
    print(f"ëª¨ë¸ ì •ë³´:")
    print(f"  ì…ë ¥ shape: {model.input_shape}")
    print(f"  ì¶œë ¥ shape: {model.output_shape}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nONNX ë³€í™˜ ì¤‘: {model_path} -> {output_path}")
    print(f"  Opset ë²„ì „: {opset}")
    
    # ONNXë¡œ ë³€í™˜
    if keras2onnx:
        # keras2onnx ì‚¬ìš©
        onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=opset)
        onnx.save_model(onnx_model, output_path)
    else:
        # tf2onnx ì‚¬ìš©
        spec = (tf.TensorSpec(model.input_shape, tf.float32, name="input"),)
        output_path_onnx = tf2onnx.convert.from_keras(
            model,
            input_signature=spec,
            opset=opset,
            output_path=output_path
        )
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    size = os.path.getsize(output_path)
    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
    print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {size:,} bytes ({size/1024:.2f} KB)")
    
    # ONNX ëª¨ë¸ ê²€ì¦
    try:
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        print("âœ… ONNX ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  ONNX ëª¨ë¸ ê²€ì¦ ì¤‘ ê²½ê³ : {e}")
    
    return output_path


def main():
    parser = argparse.ArgumentParser(description='Keras ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜')
    parser.add_argument('--model', type=str, default='model.h5',
                        help='ì…ë ¥ Keras ëª¨ë¸ íŒŒì¼ (default: model.h5)')
    parser.add_argument('--output', type=str, default='public/model/model.onnx',
                        help='ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ (default: public/model/model.onnx)')
    parser.add_argument('--opset', type=int, default=13,
                        help='ONNX opset ë²„ì „ (default: 13)')
    
    args = parser.parse_args()
    
    convert_to_onnx(args.model, args.output, args.opset)
    
    print(f"\nğŸ“ ONNX.js ì‚¬ìš© ë°©ë²•:")
    print(f"   npm install onnxruntime-web")
    print(f"   ë˜ëŠ”")
    print(f"   <script src='https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js'></script>")
    print(f"\n   ëª¨ë¸ ë¡œë“œ:")
    print(f"   const session = await ort.InferenceSession.create('./model/model.onnx');")


if __name__ == '__main__':
    main()

